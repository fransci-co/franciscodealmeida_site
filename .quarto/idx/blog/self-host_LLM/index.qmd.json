{"title":"Setting up a self-hosted LLM chatbot ft. DeepSeek","markdown":{"yaml":{"title":"Setting up a self-hosted LLM chatbot ft. DeepSeek","format":{"html":{"code-fold":true,"link-external-newwindow":true}},"jupyter":"python3","image":"img/post_llms_chat.png","description":"Self-host a model, serve, provide an UI, and expose it.","date":"2025-03-06","categories":["LLMs"]},"headingText":"DeepSeek","containsRefs":false,"markdown":"\n![*Canva GenAI, prompt: \"Minimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere.\"*](img/extended_post_llms.png)\n\n<br>\n\nOver the past few days Iâ€™ve been diving into multiple LLM frameworks, exploring the best ways to deploy a model on my own machine with the goal of setting up an AI inference chatbot for free that I can use any-time and anywhere and even make it available to others. \n\nI am very proud of my 3-year-old laptop that ended up hosting a deepseek model on his gpu and hold up quite well!  \n\nIf you search this topic, you will find many tutorials on how to run models locally, setup a LLM in known cloud services or even use API providers that hosts LLMs in their proprietary setup\n\nOn this post I summarize the most common approaches I came across, the limitations I've found and different setup iterations depending on your needs. I also walk through how I self-hosted a model, served it, gave-it an UI and exposed it. Honestly, it's quite easy, there are so many great resources available. \n\n<br>\n\n## Base Models\n\n<img height=\"64\" src=\"https://unpkg.com/@lobehub/icons-static-svg@latest/icons/deepseek-color.svg\" />\n\n\nWhat's the buzz surrounding DeepSeek all about? It delivers strong performance, occasionally outpacing competitors while consistently holding its ownâ€”but what truly distinguishes it from previous models?\n\nIn a nutshell [Deepseek R1-Zero / R1](https://arxiv.org/abs/2501.12948) is introduced as the **first-generation reasoning models**, unlike the competitors these models articulate their reasoning behind every answer, step by step. This **Chain-of-Tought (CoT)** is great not only for the user but also for the model which is aware of the reasoning and its capable of learn and correct it if needed.  By applying **Reinforcement Learning (RL)** the model gets better over time, trough experimentation and evaluation DeepSeek models are capable of improving their reasoning and update their behaviour. As a result, the need for massive amounts of labelled data is also reduced.\n\nHere are two quotes from DeepSeek: \n\n<blockquote>\n<p> *We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.* <p> \n\n<p>  *We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.* <p> \n\n <p> [*Hugging Face DeepSeek-AI*](https://huggingface.co/deepseek-ai/DeepSeek-R1) </blockquote>\n\n\n**Summarizing R1:** uses a hybrid approach, employes **Group Relative Policy Optimization (GRPO**) as the optimization policy (**RL**), utilizes cold-start data in its initial training phase (**SFT**), and undergoes additional refinement stages.\n\n\n\nDid I mention that **DeepSeek-R1** is open source? A heads-upâ€”if youâ€™re thinking you can run the full model on your own machine, think again. \n  \nLike other massive AI models, **DeepSeek-R1 671B** (with 671 billion parameters) requires a lot of computing power. Even though it doesnâ€™t activate all 671 billion parameters at once, it still demands significant resources due to its sheer scale. To improve efficiency, it uses a **Mixture-of-Experts (MoE)** architecture, activating only 37 billion parameters per request. On top of that, it incorporates large-scale reinforcement learning and other optimizations that further enhance performance and efficiency. \n\n\nAnd by \"a lot,\" I donâ€™t mean that much if you know what you're doing:\n\n<blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\" >Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. All download and part links below:</p>&mdash; Matthew Carrigan (@carrigmat) <a href=\"https://twitter.com/carrigmat/status/1884244369907278106?ref_src=twsrc%5Etfw\">January 28, 2025</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n\n::: {.callout-warning}\nTraining it from scratch is a whole different $5.6 million story-[DeepSeek-V3 Technical Report](https://arxiv.org/html/2412.19437v1).\n:::\n\n\n## Distillation \n\nSo that's when **distillation** comes in hand. **Distillation is a machine learning technique that involves transferring knowledge from a large model to a smaller one, thus making it less demanding while trying to achieve similar performance.**\n\nThese more efficient smaller models can still achieve near state-of-the-art performance for specific tasks, while solving high cost and complexity challenges of deploying Large Language Models in real-world scenarios  (@tbl-1).\n\n\nHere's another quote from DeepSeek:\n\n<blockquote>\n<p> *We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distil better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.*  <p> \n<p> \n</blockquote>\n\n<br>\n\n\n:::  {#tbl-1}\n\n```{python}\n#| column: body-outset\n#| echo: false\n\n\n# Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Data\ndata = [\n    [\"GPT-4o-0513\", 9.3, 13.4, 74.6, 49.9, 32.9, 759],\n    [\"Claude-3.5-Sonnet-1022\", 16.0, 26.7, 78.3, 65.0, 38.9, 717],\n    [\"o1-mini\", 63.6, 80.0, 90.0, 60.0, 53.8, 1820],\n    [\"QwQ-32B-Preview\", 44.0, 60.0, 90.6, 54.5, 41.9, 1316],\n    [\"DeepSeek-R1-Distill-Qwen-1.5B\", 28.9, 52.7, 83.9, 33.8, 16.9, 954],\n    [\"DeepSeek-R1-Distill-Qwen-7B\", 55.5, 83.3, 92.8, 49.1, 37.6, 1189],\n    [\"DeepSeek-R1-Distill-Qwen-14B\", 69.7, 80.0, 93.9, 59.1, 53.1, 1481],\n    [\"DeepSeek-R1-Distill-Qwen-32B\", 72.6, 83.3, 94.3, 62.1, 57.2, 1691],\n    [\"DeepSeek-R1-Distill-Llama-8B\", 50.4, 80.0, 89.1, 49.0, 39.6, 1205],\n    [\"DeepSeek-R1-Distill-Llama-70B\", 70.0, 86.7, 94.5, 65.2, 57.5, 1633]\n]\n\n# Column names\ncolumns = [\n    \"Model\", \"AIME 2024 pass@1\", \"AIME 2024 cons@64\", \"MATH-500 pass@1\", \n    \"GPQA Diamond pass@1\", \"LiveCodeBench pass@1\", \"CodeForces rating\"\n]\n\n# Create DataFrame\ndf = pd.DataFrame(data, columns=columns).set_index(\"Model\")\n\ndf.style.highlight_max(axis=0, props='color:white; font-weight:bold; background-color:darkblue;').format(precision=1)\n\n```\n\n Evaluation on distilled models [*Hugging Face DeepSeek-AI*](https://huggingface.co/deepseek-ai/DeepSeek-R1) {.striped .hover}\n\n:::\n\nI tried running the DeepSeek-R1-Distill-Llama-8B but it was a bit to slow for me, so I settled for the DeepSeek-R1-Distill-Qwen-7B. For reference my machine is a Legion 5 (Lenovo) laptop with a Nvidia RTX 3060 (6GB VRAM). \n\n\n<br>\n\n#  Serving & Hosting\n\n##  Hosting an LLM \n\nThere are three main options for hosting an LLM: \n\n   **1.**  Set up and run it on your own machine.\n\n   **2.**  Set it up on a cloud service.\n\n   **3.**  Use an API provider with their proprietary setup.\n\nMy plan was to set everything up myself and I was particularly interested in running it on my own computer. Still, I explored some cloud optionsâ€”provided they were freeâ€”as a potential alternative.\n\n\n### Cloud Services\n\nThe three major cloud services: Azure, AWS, and GCP offer similar free tiers, typically including around $300 in credits and a limited number of usage hours. For example, AWS EC2â€™s free tier provides up to 750 hours but is limited to small instances with 2 vCPUs and 1 GiB of memory, which aren't powerful enough for this project.\n\n\nOracle Cloudâ€™s \"Always Free\" tier is a much more promising option, offering ARM-based virtual machines with up to 4 cores, 24 GB of RAM, and 200 GB of storage. Even so, my priority was testing on my own machine, so I stuck with that. Regardless, itâ€™s definitely worth keeping in mind for future projects!\n\n\n### API Providers\n\nAs for API providers, **Groq** is quite appealing for personal use. It allows up to 1,000 requests per day for free on the DeepSeek-R1-Distill-LLaMA-70B a fairly big model with impressive benchmark results. It integrates easily with development frameworks like **LangChain** for building applications and provides a straightforward way to create user interfaces using **Gradio** or **Streamlit**, plus the speed is one of its key selling points.\n\n **Hugging Face** also has *Spaces* hardware and *Inference endpoint* solutions, where the first allows for hardware rental in the form of a development space and the second for the deployment of your applications. \n\n<br>\n\n\n## LLM Inference Serving\n\n Serving LLM-based applications involves two key components: the **engine** and the **server**. The engine manages model execution and request batching, while the server handles routing user requests.\n\n\n![Fig. 1 - *\"Architecture of servers and engines\"*  Source: [RunAi](https://www.run.ai/blog/serving-large-language-models) ](img/image_run_ai.png){.lightbox}\n\n\n### Engines\n\nEngines are responsible for running the models and generating text using various optimization techniques. At their core, they are typically built with Python or C++ libraries. They process incoming user requests in batches and generate responses accordingly.\n\n\n### Server \n\nThe servers orchestrate HTTP/gRPC requests from users. In real-world applications, users interact with the chatbot at different times, the server queues these requests and sends them to the engine for response generation. Additionally, it monitors important performance metrics such as throughput and latency, essential for optimizing model serving.\n\n \nFor more on serving here's a [RunAi article](https://www.run.ai/blog/serving-large-language-models).\n\n\nChoosing the right inference backend for serving LLMs plays a critical role in achieving fast generation speeds for a smooth user experience, while also boosting cost efficiency through high token throughput and optimized resource usage. With a wide range of inference engines and servers available from leading research and industry teams, selecting the best fit for a specific use case can be a challenging task.\n\nPopular open-source tools for inference serving:\n\n\n- **Triton Inference Server** &  **TensorRT-LLM** - NVIDIA\n\n- **vLLM** - Originally Sky Computing Lab at UC Berkeley has evolved into a community-driven project with contributions from both academia and industry.\n\n- **TGI** - Hugging Face\n\n- **Ollama** - Community driven.\n\n- **Aphrodite-Engine** -  PygmalionAI & Ruliad\n\n- **LMDeploy** - MMRazor & MMDeploy\n\n- **SGLang** - Backed by an active community with industry adoption.\n\n- **llama.cpp** - Started from ggml.ai \n\n- **RayLLM & RayServe** - Anyscale\n \n\n\n### Ollama \n<img height=\"64\" src=\"https://unpkg.com/@lobehub/icons-static-svg@latest/icons/ollama.svg\" />\n\nI went with Ollama for this project due to its simplicity, accessibility, and smooth integration with various frameworksâ€”ensuring it'll be straightforward to implement future features, such as the frontend which I'll discuss in the next section. \n\n::: {#tip1 .callout-tip}\n## Tip\nIâ€™ve already looked into it beforehand and I know itâ€™ll make setting up fine-tuning with RAG and Unsloth-AI in a future project much easier. ðŸ˜Ž\n:::\n\n\n\nOllama provides a constantly updated library of pre-trained LLM models, while ensuring effortless model management eliminating the complexities of model formats and dependencies. While it may not be the most scalable solution for large enterprises and can be slower than some alternatives, it significantly simplifies environment setup and overall workflow.\n\nIt's built on top of **llama.cpp** and employs a Client-Server architecture, where the client interacts with the user via the command line, and communication between the client, server, and engine happens over HTTP. The server can be started through the command line, a desktop application or docker. Regardless of the method they all invoke the same executable file.\n\n\n\n![Fig. 2- General Overview of Ollama. [Adapted](https://medium.com/@rifewang/analysis-of-ollama-architecture-and-conversation-processing-flow-for-ai-llm-tool-ead4b9f40975) ](img/image_ollama_general.png){.lightbox}\n\n<br>\n\n## User Interface (UI)\n\nNow, we need a way to interact with our chatbot beyond the command line. The good news is that there are many open-source platforms with built-in interfaces that we can easily connect to our service. This means we donâ€™t need to be developers to have an attractive and functional interface. In fact, the available solutions are so polished that thereâ€™s really no reason to build something from scratch that wouldnâ€™t match the quality of these options.\n\n\nSome examples are: \n\n- **OpenWebUI**\n\n- **HuggingChat**\n\n- **AnythingLLM**\n\n- **LibreChat**\n\n- **Jan**\n\n- **Text Generation WebUI**\n\nthe list could go on and on...\n\n\n### Open WebUI\n\nI chose OpenWeb UI, and honestly, there's not much to sayâ€”it's just a fantastic tool all around. The interface is clean and intuitive, setting it up is easy, it offers extensive customization and features, it supports multiple models and backend and integrates advanced functionalities. \n\n\nOpen WebUI is a community driven self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for **Retrieval Augmented Generation (RAG)**, making it a powerful AI deployment solution.\n\n\n![Fig. 3- Demo [OpenWebUI](https://github.com/open-webui/open-webui).](gif/demo.gif){.lightbox} \n\n<br>\n\n\n#  Exposing \n## Tunneling Tools \n\nA tunneling tool allows you to expose a local service (running on your computer or private network) to the internet. It creates a secure tunnel between your local machine and a public URL, letting others access your local service without needing to configure complex networking settings like port forwarding or firewall rules.\n\nHow it works:\n\n  **1.** The tunneling tool runs on your local machine and connects to an external server.\n\n  **2.** The external server provides a public URL (often temporary).\n\n  **3.** Requests to the public URL are forwarded through the tunnel to your local service.\n\n\n### ngrok\n\nNgrok is one of the most widely used tunneling tools, offering a quick and easy way to expose local services to the internet. With a single command like `ngrok http 3000`, it generates a public URL that forwards traffic to your local server, making it ideal for testing webhooks, remote access, and development. Paid plans offer additional features like custom domains, authentication, and enhanced security. \n\nFor more information on the topic specifically for our use case here's a nice [piece on ngrok blog](https://ngrok.com/blog-post/unlock-remote-ai-power-with-ngrok-a-game-changer-for-developers) worth checking out.\n\n\n::: {.callout-warning}\n## Warning\nIn this post we are skipping security best-practices, to learn how to ensure you are using **ngrok** securely please check their [documentation](https://ngrok.com/docs/guides/other-guides/securing-your-tunnels/). You can also setup permissions and user groups on [Open WebUI](https://docs.openwebui.com/getting-started/env-configuration#chat-permissions). \n\n:::\n\n<br>\n\n# Hands-on\n---\n\nWhen I first envisioned this project, I thought it would be a great way to sharpen my OOP skills in Python. Funny enough, I ended up not writing a single line of Python.\nBoth Ollama and OpenWebUI provide maintained Docker images, so setting them up is as simple as running their respective containers and configuring communication between them via endpoints. Container orchestration is handled using Docker Compose.\n\nIf youâ€™re planning to run the model on a GPU, youâ€™ll need to configure the enviroment and install the NVIDIA drivers. This process can be easily automated using `post-create command` in your `docker-compose.yaml` or `dockerfile`.\n\n>ðŸš€ Iâ€™ll be sharing the full code soon, but first, I want to play around with fine-tuning and see if I can integrate that into the project. **Stay tuned for an update in my next post!**\n\n<br>\n\n\n\nI'm using VS Code on WSL, so I'll be referring to Linux commands.  With VS Code you can also easily set up a DevContainer for testing, experimenting with different frameworks, or simply test things out.\n\nThe first step is to ensure you have Docker (or a compatible container runtime).\n\nThen, if you're running the model on your NVIDIA GPU, to set up your environment for GPU acceleration use the following commands (installing with apt): \n\n::: {style=\"font-size: 75%;\"}\nSource: [Ollama documentation](https://hub.docker.com/r/ollama/ollama)\n:::\n\n\n**1.** Configure the repository\n\n```{.bash}\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\n```\n\n\n**2.** Install the [NVIDIA Container Toolkit packages.](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation)\n\n```{.bash}\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n\nAs for docker compose all you need to do is setup both services, for example as following: \n\n```{.yaml filename=\"docker-compose.yaml\" eval=FALSE}\nservices:\n\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n      - 7869:11434\n    volumes:\n      - .:/workspace\n      - ./ollama/ollama:/root/.ollama\n    container_name: ollama\n    pull_policy: always\n    tty: true\n    restart: always\n    environment:\n      - OLLAMA_KEEP_ALIVE=24h\n    networks:\n      - llm_inference\n    deploy:  #Only if you're running with GPU\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n\n  open-webui:\n    image:  ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    volumes:\n      - ./open-webui:/app/backend/data\n    depends_on:\n      - ollama\n    ports:\n      - 8080:8080\n    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models\n      - OLLAMA_BASE_URLS=http://host.docker.internal:7869 \n      - ENV=dev\n      - WEBUI_AUTH=False\n      - WEBUI_URL=http://localhost:8080\n      - WEBUI_SECRET_KEY= wg55pp #random secret key\n\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    restart: unless-stopped\n    networks:\n      - llm_inference\n\nnetworks:\n  llm_inference:\n    external: false\n\nvolumes:\n  workspace:\n    driver: local\n```\n\n\n\nTo download the model, you can interact directly with the container and pull the model. You can easily check all available models on [Ollama's site](https://ollama.com/search).\n\n```{.bash}\ndocker exec -it ollama run deepseek-r1:7b\n```\n\nNow that everything is set up, we can simply run all the services and expose the port used by OpenWebUI through a tunneling tool, in this case ngrok. Some other options and providers are covered on [Ollama's FAQ.](https://github.com/ollama/ollama/blob/main/docs/faq.md)\n\n```{.bash}\nngrok http 8080 --host-header=\"localhost:8080\"\n```\n\n\n\nIf we were to scale this for multiple instances/users, Kubernetes should work with a very similar setup. I came across a helpful post on Medium that explains how to do it [Host Your Own Ollama Service in a Cloud Kubernetes (K8s) Cluster](https://medium.com/@yuxiaojian/host-your-own-ollama-service-in-a-cloud-kubernetes-k8s-cluster-c818ca84a055)\nI havenâ€™t read it thoroughly, but I think itâ€™s worth noting.\n\n<br>\n\n\n\n### Closing Notes\n\nSince I started writing this post in late February and published it on 6th of March, three more models utilizing reinforcement learning have been released. One is [QwQ-32B](https://qwenlm.github.io/blog/qwq-32b/) (just today!), and the other is [Grok 3](https://x.ai/blog/grok-3), both setting new benchmark records. The first is open-source, and there are expectations that Grok 3 will follow, probably in the future when a new model from X replaces it. [GPT-4.5](https://openai.com/index/gpt-4-5-system-card/) is also available as a preview for paid subscribers and uses reinforcement learning as well.  \n\nDeepSeekâ€™s work with reinforcement learning has laid the foundation for a new approach to LLMsâ€”one that emphasizes both reinforcement learning and open-source access.  \n\nThe future looks bright, and these advancements should be within reach for anyone eager to embrace this journey. Itâ€™s exciting to witness how quickly progress is being made with more and better open-source tools emerging every day. \n\n<br>\n\n>*I hope this post provides a clear overview on inference basics*\n\n\n","srcMarkdownNoYaml":"\n![*Canva GenAI, prompt: \"Minimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere.\"*](img/extended_post_llms.png)\n\n<br>\n\nOver the past few days Iâ€™ve been diving into multiple LLM frameworks, exploring the best ways to deploy a model on my own machine with the goal of setting up an AI inference chatbot for free that I can use any-time and anywhere and even make it available to others. \n\nI am very proud of my 3-year-old laptop that ended up hosting a deepseek model on his gpu and hold up quite well!  \n\nIf you search this topic, you will find many tutorials on how to run models locally, setup a LLM in known cloud services or even use API providers that hosts LLMs in their proprietary setup\n\nOn this post I summarize the most common approaches I came across, the limitations I've found and different setup iterations depending on your needs. I also walk through how I self-hosted a model, served it, gave-it an UI and exposed it. Honestly, it's quite easy, there are so many great resources available. \n\n<br>\n\n# DeepSeek \n## Base Models\n\n<img height=\"64\" src=\"https://unpkg.com/@lobehub/icons-static-svg@latest/icons/deepseek-color.svg\" />\n\n\nWhat's the buzz surrounding DeepSeek all about? It delivers strong performance, occasionally outpacing competitors while consistently holding its ownâ€”but what truly distinguishes it from previous models?\n\nIn a nutshell [Deepseek R1-Zero / R1](https://arxiv.org/abs/2501.12948) is introduced as the **first-generation reasoning models**, unlike the competitors these models articulate their reasoning behind every answer, step by step. This **Chain-of-Tought (CoT)** is great not only for the user but also for the model which is aware of the reasoning and its capable of learn and correct it if needed.  By applying **Reinforcement Learning (RL)** the model gets better over time, trough experimentation and evaluation DeepSeek models are capable of improving their reasoning and update their behaviour. As a result, the need for massive amounts of labelled data is also reduced.\n\nHere are two quotes from DeepSeek: \n\n<blockquote>\n<p> *We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.* <p> \n\n<p>  *We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.* <p> \n\n <p> [*Hugging Face DeepSeek-AI*](https://huggingface.co/deepseek-ai/DeepSeek-R1) </blockquote>\n\n\n**Summarizing R1:** uses a hybrid approach, employes **Group Relative Policy Optimization (GRPO**) as the optimization policy (**RL**), utilizes cold-start data in its initial training phase (**SFT**), and undergoes additional refinement stages.\n\n\n\nDid I mention that **DeepSeek-R1** is open source? A heads-upâ€”if youâ€™re thinking you can run the full model on your own machine, think again. \n  \nLike other massive AI models, **DeepSeek-R1 671B** (with 671 billion parameters) requires a lot of computing power. Even though it doesnâ€™t activate all 671 billion parameters at once, it still demands significant resources due to its sheer scale. To improve efficiency, it uses a **Mixture-of-Experts (MoE)** architecture, activating only 37 billion parameters per request. On top of that, it incorporates large-scale reinforcement learning and other optimizations that further enhance performance and efficiency. \n\n\nAnd by \"a lot,\" I donâ€™t mean that much if you know what you're doing:\n\n<blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\" >Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. All download and part links below:</p>&mdash; Matthew Carrigan (@carrigmat) <a href=\"https://twitter.com/carrigmat/status/1884244369907278106?ref_src=twsrc%5Etfw\">January 28, 2025</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n\n::: {.callout-warning}\nTraining it from scratch is a whole different $5.6 million story-[DeepSeek-V3 Technical Report](https://arxiv.org/html/2412.19437v1).\n:::\n\n\n## Distillation \n\nSo that's when **distillation** comes in hand. **Distillation is a machine learning technique that involves transferring knowledge from a large model to a smaller one, thus making it less demanding while trying to achieve similar performance.**\n\nThese more efficient smaller models can still achieve near state-of-the-art performance for specific tasks, while solving high cost and complexity challenges of deploying Large Language Models in real-world scenarios  (@tbl-1).\n\n\nHere's another quote from DeepSeek:\n\n<blockquote>\n<p> *We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distil better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.*  <p> \n<p> \n</blockquote>\n\n<br>\n\n\n:::  {#tbl-1}\n\n```{python}\n#| column: body-outset\n#| echo: false\n\n\n# Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Data\ndata = [\n    [\"GPT-4o-0513\", 9.3, 13.4, 74.6, 49.9, 32.9, 759],\n    [\"Claude-3.5-Sonnet-1022\", 16.0, 26.7, 78.3, 65.0, 38.9, 717],\n    [\"o1-mini\", 63.6, 80.0, 90.0, 60.0, 53.8, 1820],\n    [\"QwQ-32B-Preview\", 44.0, 60.0, 90.6, 54.5, 41.9, 1316],\n    [\"DeepSeek-R1-Distill-Qwen-1.5B\", 28.9, 52.7, 83.9, 33.8, 16.9, 954],\n    [\"DeepSeek-R1-Distill-Qwen-7B\", 55.5, 83.3, 92.8, 49.1, 37.6, 1189],\n    [\"DeepSeek-R1-Distill-Qwen-14B\", 69.7, 80.0, 93.9, 59.1, 53.1, 1481],\n    [\"DeepSeek-R1-Distill-Qwen-32B\", 72.6, 83.3, 94.3, 62.1, 57.2, 1691],\n    [\"DeepSeek-R1-Distill-Llama-8B\", 50.4, 80.0, 89.1, 49.0, 39.6, 1205],\n    [\"DeepSeek-R1-Distill-Llama-70B\", 70.0, 86.7, 94.5, 65.2, 57.5, 1633]\n]\n\n# Column names\ncolumns = [\n    \"Model\", \"AIME 2024 pass@1\", \"AIME 2024 cons@64\", \"MATH-500 pass@1\", \n    \"GPQA Diamond pass@1\", \"LiveCodeBench pass@1\", \"CodeForces rating\"\n]\n\n# Create DataFrame\ndf = pd.DataFrame(data, columns=columns).set_index(\"Model\")\n\ndf.style.highlight_max(axis=0, props='color:white; font-weight:bold; background-color:darkblue;').format(precision=1)\n\n```\n\n Evaluation on distilled models [*Hugging Face DeepSeek-AI*](https://huggingface.co/deepseek-ai/DeepSeek-R1) {.striped .hover}\n\n:::\n\nI tried running the DeepSeek-R1-Distill-Llama-8B but it was a bit to slow for me, so I settled for the DeepSeek-R1-Distill-Qwen-7B. For reference my machine is a Legion 5 (Lenovo) laptop with a Nvidia RTX 3060 (6GB VRAM). \n\n\n<br>\n\n#  Serving & Hosting\n\n##  Hosting an LLM \n\nThere are three main options for hosting an LLM: \n\n   **1.**  Set up and run it on your own machine.\n\n   **2.**  Set it up on a cloud service.\n\n   **3.**  Use an API provider with their proprietary setup.\n\nMy plan was to set everything up myself and I was particularly interested in running it on my own computer. Still, I explored some cloud optionsâ€”provided they were freeâ€”as a potential alternative.\n\n\n### Cloud Services\n\nThe three major cloud services: Azure, AWS, and GCP offer similar free tiers, typically including around $300 in credits and a limited number of usage hours. For example, AWS EC2â€™s free tier provides up to 750 hours but is limited to small instances with 2 vCPUs and 1 GiB of memory, which aren't powerful enough for this project.\n\n\nOracle Cloudâ€™s \"Always Free\" tier is a much more promising option, offering ARM-based virtual machines with up to 4 cores, 24 GB of RAM, and 200 GB of storage. Even so, my priority was testing on my own machine, so I stuck with that. Regardless, itâ€™s definitely worth keeping in mind for future projects!\n\n\n### API Providers\n\nAs for API providers, **Groq** is quite appealing for personal use. It allows up to 1,000 requests per day for free on the DeepSeek-R1-Distill-LLaMA-70B a fairly big model with impressive benchmark results. It integrates easily with development frameworks like **LangChain** for building applications and provides a straightforward way to create user interfaces using **Gradio** or **Streamlit**, plus the speed is one of its key selling points.\n\n **Hugging Face** also has *Spaces* hardware and *Inference endpoint* solutions, where the first allows for hardware rental in the form of a development space and the second for the deployment of your applications. \n\n<br>\n\n\n## LLM Inference Serving\n\n Serving LLM-based applications involves two key components: the **engine** and the **server**. The engine manages model execution and request batching, while the server handles routing user requests.\n\n\n![Fig. 1 - *\"Architecture of servers and engines\"*  Source: [RunAi](https://www.run.ai/blog/serving-large-language-models) ](img/image_run_ai.png){.lightbox}\n\n\n### Engines\n\nEngines are responsible for running the models and generating text using various optimization techniques. At their core, they are typically built with Python or C++ libraries. They process incoming user requests in batches and generate responses accordingly.\n\n\n### Server \n\nThe servers orchestrate HTTP/gRPC requests from users. In real-world applications, users interact with the chatbot at different times, the server queues these requests and sends them to the engine for response generation. Additionally, it monitors important performance metrics such as throughput and latency, essential for optimizing model serving.\n\n \nFor more on serving here's a [RunAi article](https://www.run.ai/blog/serving-large-language-models).\n\n\nChoosing the right inference backend for serving LLMs plays a critical role in achieving fast generation speeds for a smooth user experience, while also boosting cost efficiency through high token throughput and optimized resource usage. With a wide range of inference engines and servers available from leading research and industry teams, selecting the best fit for a specific use case can be a challenging task.\n\nPopular open-source tools for inference serving:\n\n\n- **Triton Inference Server** &  **TensorRT-LLM** - NVIDIA\n\n- **vLLM** - Originally Sky Computing Lab at UC Berkeley has evolved into a community-driven project with contributions from both academia and industry.\n\n- **TGI** - Hugging Face\n\n- **Ollama** - Community driven.\n\n- **Aphrodite-Engine** -  PygmalionAI & Ruliad\n\n- **LMDeploy** - MMRazor & MMDeploy\n\n- **SGLang** - Backed by an active community with industry adoption.\n\n- **llama.cpp** - Started from ggml.ai \n\n- **RayLLM & RayServe** - Anyscale\n \n\n\n### Ollama \n<img height=\"64\" src=\"https://unpkg.com/@lobehub/icons-static-svg@latest/icons/ollama.svg\" />\n\nI went with Ollama for this project due to its simplicity, accessibility, and smooth integration with various frameworksâ€”ensuring it'll be straightforward to implement future features, such as the frontend which I'll discuss in the next section. \n\n::: {#tip1 .callout-tip}\n## Tip\nIâ€™ve already looked into it beforehand and I know itâ€™ll make setting up fine-tuning with RAG and Unsloth-AI in a future project much easier. ðŸ˜Ž\n:::\n\n\n\nOllama provides a constantly updated library of pre-trained LLM models, while ensuring effortless model management eliminating the complexities of model formats and dependencies. While it may not be the most scalable solution for large enterprises and can be slower than some alternatives, it significantly simplifies environment setup and overall workflow.\n\nIt's built on top of **llama.cpp** and employs a Client-Server architecture, where the client interacts with the user via the command line, and communication between the client, server, and engine happens over HTTP. The server can be started through the command line, a desktop application or docker. Regardless of the method they all invoke the same executable file.\n\n\n\n![Fig. 2- General Overview of Ollama. [Adapted](https://medium.com/@rifewang/analysis-of-ollama-architecture-and-conversation-processing-flow-for-ai-llm-tool-ead4b9f40975) ](img/image_ollama_general.png){.lightbox}\n\n<br>\n\n## User Interface (UI)\n\nNow, we need a way to interact with our chatbot beyond the command line. The good news is that there are many open-source platforms with built-in interfaces that we can easily connect to our service. This means we donâ€™t need to be developers to have an attractive and functional interface. In fact, the available solutions are so polished that thereâ€™s really no reason to build something from scratch that wouldnâ€™t match the quality of these options.\n\n\nSome examples are: \n\n- **OpenWebUI**\n\n- **HuggingChat**\n\n- **AnythingLLM**\n\n- **LibreChat**\n\n- **Jan**\n\n- **Text Generation WebUI**\n\nthe list could go on and on...\n\n\n### Open WebUI\n\nI chose OpenWeb UI, and honestly, there's not much to sayâ€”it's just a fantastic tool all around. The interface is clean and intuitive, setting it up is easy, it offers extensive customization and features, it supports multiple models and backend and integrates advanced functionalities. \n\n\nOpen WebUI is a community driven self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for **Retrieval Augmented Generation (RAG)**, making it a powerful AI deployment solution.\n\n\n![Fig. 3- Demo [OpenWebUI](https://github.com/open-webui/open-webui).](gif/demo.gif){.lightbox} \n\n<br>\n\n\n#  Exposing \n## Tunneling Tools \n\nA tunneling tool allows you to expose a local service (running on your computer or private network) to the internet. It creates a secure tunnel between your local machine and a public URL, letting others access your local service without needing to configure complex networking settings like port forwarding or firewall rules.\n\nHow it works:\n\n  **1.** The tunneling tool runs on your local machine and connects to an external server.\n\n  **2.** The external server provides a public URL (often temporary).\n\n  **3.** Requests to the public URL are forwarded through the tunnel to your local service.\n\n\n### ngrok\n\nNgrok is one of the most widely used tunneling tools, offering a quick and easy way to expose local services to the internet. With a single command like `ngrok http 3000`, it generates a public URL that forwards traffic to your local server, making it ideal for testing webhooks, remote access, and development. Paid plans offer additional features like custom domains, authentication, and enhanced security. \n\nFor more information on the topic specifically for our use case here's a nice [piece on ngrok blog](https://ngrok.com/blog-post/unlock-remote-ai-power-with-ngrok-a-game-changer-for-developers) worth checking out.\n\n\n::: {.callout-warning}\n## Warning\nIn this post we are skipping security best-practices, to learn how to ensure you are using **ngrok** securely please check their [documentation](https://ngrok.com/docs/guides/other-guides/securing-your-tunnels/). You can also setup permissions and user groups on [Open WebUI](https://docs.openwebui.com/getting-started/env-configuration#chat-permissions). \n\n:::\n\n<br>\n\n# Hands-on\n---\n\nWhen I first envisioned this project, I thought it would be a great way to sharpen my OOP skills in Python. Funny enough, I ended up not writing a single line of Python.\nBoth Ollama and OpenWebUI provide maintained Docker images, so setting them up is as simple as running their respective containers and configuring communication between them via endpoints. Container orchestration is handled using Docker Compose.\n\nIf youâ€™re planning to run the model on a GPU, youâ€™ll need to configure the enviroment and install the NVIDIA drivers. This process can be easily automated using `post-create command` in your `docker-compose.yaml` or `dockerfile`.\n\n>ðŸš€ Iâ€™ll be sharing the full code soon, but first, I want to play around with fine-tuning and see if I can integrate that into the project. **Stay tuned for an update in my next post!**\n\n<br>\n\n\n\nI'm using VS Code on WSL, so I'll be referring to Linux commands.  With VS Code you can also easily set up a DevContainer for testing, experimenting with different frameworks, or simply test things out.\n\nThe first step is to ensure you have Docker (or a compatible container runtime).\n\nThen, if you're running the model on your NVIDIA GPU, to set up your environment for GPU acceleration use the following commands (installing with apt): \n\n::: {style=\"font-size: 75%;\"}\nSource: [Ollama documentation](https://hub.docker.com/r/ollama/ollama)\n:::\n\n\n**1.** Configure the repository\n\n```{.bash}\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update\n```\n\n\n**2.** Install the [NVIDIA Container Toolkit packages.](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation)\n\n```{.bash}\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n\nAs for docker compose all you need to do is setup both services, for example as following: \n\n```{.yaml filename=\"docker-compose.yaml\" eval=FALSE}\nservices:\n\n  ollama:\n    image: ollama/ollama:latest\n    ports:\n      - 7869:11434\n    volumes:\n      - .:/workspace\n      - ./ollama/ollama:/root/.ollama\n    container_name: ollama\n    pull_policy: always\n    tty: true\n    restart: always\n    environment:\n      - OLLAMA_KEEP_ALIVE=24h\n    networks:\n      - llm_inference\n    deploy:  #Only if you're running with GPU\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n\n  open-webui:\n    image:  ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    volumes:\n      - ./open-webui:/app/backend/data\n    depends_on:\n      - ollama\n    ports:\n      - 8080:8080\n    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models\n      - OLLAMA_BASE_URLS=http://host.docker.internal:7869 \n      - ENV=dev\n      - WEBUI_AUTH=False\n      - WEBUI_URL=http://localhost:8080\n      - WEBUI_SECRET_KEY= wg55pp #random secret key\n\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    restart: unless-stopped\n    networks:\n      - llm_inference\n\nnetworks:\n  llm_inference:\n    external: false\n\nvolumes:\n  workspace:\n    driver: local\n```\n\n\n\nTo download the model, you can interact directly with the container and pull the model. You can easily check all available models on [Ollama's site](https://ollama.com/search).\n\n```{.bash}\ndocker exec -it ollama run deepseek-r1:7b\n```\n\nNow that everything is set up, we can simply run all the services and expose the port used by OpenWebUI through a tunneling tool, in this case ngrok. Some other options and providers are covered on [Ollama's FAQ.](https://github.com/ollama/ollama/blob/main/docs/faq.md)\n\n```{.bash}\nngrok http 8080 --host-header=\"localhost:8080\"\n```\n\n\n\nIf we were to scale this for multiple instances/users, Kubernetes should work with a very similar setup. I came across a helpful post on Medium that explains how to do it [Host Your Own Ollama Service in a Cloud Kubernetes (K8s) Cluster](https://medium.com/@yuxiaojian/host-your-own-ollama-service-in-a-cloud-kubernetes-k8s-cluster-c818ca84a055)\nI havenâ€™t read it thoroughly, but I think itâ€™s worth noting.\n\n<br>\n\n\n\n### Closing Notes\n\nSince I started writing this post in late February and published it on 6th of March, three more models utilizing reinforcement learning have been released. One is [QwQ-32B](https://qwenlm.github.io/blog/qwq-32b/) (just today!), and the other is [Grok 3](https://x.ai/blog/grok-3), both setting new benchmark records. The first is open-source, and there are expectations that Grok 3 will follow, probably in the future when a new model from X replaces it. [GPT-4.5](https://openai.com/index/gpt-4-5-system-card/) is also available as a preview for paid subscribers and uses reinforcement learning as well.  \n\nDeepSeekâ€™s work with reinforcement learning has laid the foundation for a new approach to LLMsâ€”one that emphasizes both reinforcement learning and open-source access.  \n\nThe future looks bright, and these advancements should be within reach for anyone eager to embrace this journey. Itâ€™s exciting to witness how quickly progress is being made with more and better open-source tools emerging every day. \n\n<br>\n\n>*I hope this post provides a clear overview on inference basics*\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"include-after-body":["../../assets/footer.html"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":{"light":["flatly","../../styles.scss","../../theme-light.scss"],"dark":["darkly","../../styles.scss","../../theme-dark.scss"]},"title-block-banner":false,"title":"Setting up a self-hosted LLM chatbot ft. DeepSeek","jupyter":"python3","image":"img/post_llms_chat.png","description":"Self-host a model, serve, provide an UI, and expose it.","date":"2025-03-06","categories":["LLMs"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
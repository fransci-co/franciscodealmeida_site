{
  "hash": "649d75a7dd915d2a1581b0fbb73b3053",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Self-host a chatbot based on deepseek LLM\"\nimage: \"img/post_llms_chat.png\"\ndescription: \"self-hosted a model, give-it an UI, expose and serve it\"\ndate: 2025-02-19\ncategories: [Quarto ]\n---\n\n\n\n\n\n![](img/post_llms_chat.png){fig-alt=\"Canva GenAI, prompt: \"Minimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere\"\" fig-align=\"left\" width=200 height=200}\n\nMinimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere.jpg\n\n\nOver the past few days, I’ve been diving into multiple LLM frameworks, exploring the best ways to deploy a model on my own machine with the goal of setting up an AI inference chatbot for free that I can use any-time and anywhere and even make it available to others. \nI am very proud of my 3-year old laptop that ended up hosting the deepseek-r1:7b on his gpu (nvidia 3060 rtx) and hold up quite well!\n\nIf you search this topic you will find many tutorials on how to run models locally, setup an llm in known cloud services or even use APIs providers that hosts llms in their proprietary setup. On this post I summarize the most common approaches I came across, the limitations I've found and different setup iterations depending on your needs. Check it out if you want to know how far you can go without spending a dime. \n\nI also share how I self-hosted a model, gave-it an UI, exposed and served it, honestly- it's pretty simple, there's so much good stuff avaialable nowadays.\n\n\n\n<br>\n<br>\n\n## DeepSeek\n\nFirst things first, what's all the buzz on deepseek about? We've heard that it outperforms its competitors but what's so new about it?\n\nIn a nutshell Deepseek R1-Zero / R1 is introduced as the first-generation reasoning model, unlike the competitors this models articulate their reasoning behind every answer, step by step. This chain-of-tought (CoT) is great not only for the user but also for the model which is aware of the reasoning and its capable of learn and correct it if needed.  By applying reinforcement learning (RL) the model gets better over time, trough experimentation and evaluation Deepseek is capable of improving its reasoning and update its behaviour.  This also reduces the need for massive amounts of labeled data. \n\n<br>\n<br>\n\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1\n\nHere are two quote from deepseek: \n\n> *\"We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\"*\n\n\n\n>*\"We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\"*\n\n\nDid I metioned that deepseek-r1 is open source? Spoiler alert, as you can imagine you still need a LOT of computing power to run the big models, such as the full DeepSeek-R1 671B (671 billion parameters). And even knowing that it doesn’t activate all 671 billion parameters at once, it still demands significant resources simply due to its sheer scale. To improve efficiency, it uses a Mixture-of-Experts (MoE) architecture, which selectively activates only 37 billion parameters per request. On top of that, it leverages large-scale reinforcement learning and other optimizations that further enhance performance.\n\n\nSo that's when distillation comes in handy. Distillation is a machine learning technique that involves transferring knowledge from a large model to a smaller one, thus making it less demanding while trying to achieve similar performance. These more efficient smaller models can still achieve near state-of-the-art performance for specific tasks, while solving high cost and complexity challenges of deploying Large Language Models in real-world scenarios. \n\nAnd here's another quote from deepseek:\n\n>*\"We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\"*\n\n\n Evaluation on distilled models:\n\n \n| Model                            | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|----------------------------------|-----------------|------------------|----------------|--------------------|--------------------|-----------------|\n| GPT-4o-0513                      | 9.3             | 13.4             | 74.6           | 49.9               | 32.9               | 759             |\n| Claude-3.5-Sonnet-1022           | 16.0            | 26.7             | 78.3           | 65.0               | 38.9               | 717             |\n| o1-mini                          | 63.6            | 80.0             | 90.0           | 60.0               | 53.8               | **1820**        |\n| QwQ-32B-Preview                  | 44.0            | 60.0             | 90.6           | 54.5               | 41.9               | 1316            |\n| DeepSeek-R1-Distill-Qwen-1.5B    | 28.9            | 52.7             | 83.9           | 33.8               | 16.9               | 954             |\n| DeepSeek-R1-Distill-Qwen-7B      | 55.5            | 83.3             | 92.8           | 49.1               | 37.6               | 1189            |\n| DeepSeek-R1-Distill-Qwen-14B     | 69.7            | 80.0             | 93.9           | 59.1               | 53.1               | 1481            |\n| DeepSeek-R1-Distill-Qwen-32B     | **72.6**        | 83.3             | 94.3           | 62.1               | 57.2               | 1691            |\n| DeepSeek-R1-Distill-Llama-8B     | 50.4            | 80.0             | 89.1           | 49.0               | 39.6               | 1205            |\n| DeepSeek-R1-Distill-Llama-70B    | 70.0            | **86.7**         | **94.5**       | **65.2**           | **57.5**           | 1633            |\n\n\n \n\n\n\n\n```{dockerfile}\n#| echo: true\n\n\n# Initialize a counter\nimport os\ncounter = 0\n\n# While loop example\nwhile counter < 5:\n    print(f\"Counter is at: {counter}\")\n    \n    # If statement inside while loop\n    if counter % 2 == 0:\n        print(\"It's an even number!\")\n\n    counter += 1  # Increment the counter\n\nprint(\"Now using a for loop:\")\n\n# For loop example\nnumbers = [1, 2, 3, 4, 5]\nfor num in numbers:\n    if num == 3:\n        print(\"Found number 3!\")\n    else:\n        print(f\"Number: {num}\")\n\nprint(\"Looping complete!\")\n\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
[
  {
    "objectID": "notes/Virtual Machines/7zip.html",
    "href": "notes/Virtual Machines/7zip.html",
    "title": "Extract Virtual Disks with 7zip",
    "section": "",
    "text": "With 7zip it’s possible to extract virtual disk files without having to mount a new VM, this also includes WSL since it also uses a virtual disk.\nhttps://www.7-zip.org/\n7-Zip 24.09 (2024-11-29)\n\nSupported formats:\nPacking / unpacking: 7z, XZ, BZIP2, GZIP, TAR, ZIP and WIM Unpacking only: APFS, AR, ARJ, CAB, CHM, CPIO, CramFS, DMG, EXT, FAT, GPT, HFS, IHEX, ISO, LZH, LZMA, MBR, MSI, NSIS, NTFS, QCOW2, RAR, RPM, SquashFS, UDF, UEFI, VDI, VHD, VHDX, VMDK, XAR and Z. For ZIP and GZIP formats, 7-Zip provides a compression ratio that is 2-10 % better than the ratio provided by PKZip and WinZip",
    "crumbs": [
      "notes",
      "Notes",
      "Virtual Machines",
      "Extract Virtual Disks with 7zip"
    ]
  },
  {
    "objectID": "notes/Virtual Machines/GPUpartioning.html",
    "href": "notes/Virtual Machines/GPUpartioning.html",
    "title": "GPU Partitioning",
    "section": "",
    "text": "GPU Partitioning on Windows laptop/desktop for VMs w/ Windows Hyper-V A compilation of information on how to do GPU Paravirtualization on windows laptop using Windows Hyper-V.\nCheck this guide: https://github.com/fransci-co/LaptopGPU_PV",
    "crumbs": [
      "notes",
      "Notes",
      "Virtual Machines",
      "GPU Partitioning"
    ]
  },
  {
    "objectID": "blog/self-host_LLM/index.html",
    "href": "blog/self-host_LLM/index.html",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "",
    "text": "Canva GenAI, prompt: “Minimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere.”\nOver the past few days I’ve been diving into multiple LLM frameworks, exploring the best ways to deploy a model on my own machine with the goal of setting up an AI inference chatbot for free that I can use any-time and anywhere and even make it available to others.\nI am very proud of my 3-year-old laptop that ended up hosting a deepseek model on his gpu and hold up quite well!\nIf you search this topic, you will find many tutorials on how to run models locally, setup a LLM in known cloud services or even use API providers that hosts LLMs in their proprietary setup\nOn this post I summarize the most common approaches I came across, the limitations I’ve found and different setup iterations depending on your needs. I also walk through how I self-hosted a model, served it, gave-it an UI and exposed it. Honestly, it’s quite easy, there are so many great resources available."
  },
  {
    "objectID": "blog/self-host_LLM/index.html#base-models",
    "href": "blog/self-host_LLM/index.html#base-models",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "Base Models",
    "text": "Base Models\n\nWhat’s the buzz surrounding DeepSeek all about? It delivers strong performance, occasionally outpacing competitors while consistently holding its own—but what truly distinguishes it from previous models?\nIn a nutshell Deepseek R1-Zero / R1 is introduced as the first-generation reasoning models, unlike the competitors these models articulate their reasoning behind every answer, step by step. This Chain-of-Tought (CoT) is great not only for the user but also for the model which is aware of the reasoning and its capable of learn and correct it if needed. By applying Reinforcement Learning (RL) the model gets better over time, trough experimentation and evaluation DeepSeek models are capable of improving their reasoning and update their behaviour. As a result, the need for massive amounts of labelled data is also reduced.\nHere are two quotes from DeepSeek:\n\n\nWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\n\n\nHugging Face DeepSeek-AI\n\nSummarizing R1: uses a hybrid approach, employes Group Relative Policy Optimization (GRPO) as the optimization policy (RL), utilizes cold-start data in its initial training phase (SFT), and undergoes additional refinement stages.\nDid I mention that DeepSeek-R1 is open source? A heads-up—if you’re thinking you can run the full model on your own machine, think again.\nLike other massive AI models, DeepSeek-R1 671B (with 671 billion parameters) requires a lot of computing power. Even though it doesn’t activate all 671 billion parameters at once, it still demands significant resources due to its sheer scale. To improve efficiency, it uses a Mixture-of-Experts (MoE) architecture, activating only 37 billion parameters per request. On top of that, it incorporates large-scale reinforcement learning and other optimizations that further enhance performance and efficiency.\nAnd by “a lot,” I don’t mean that much if you know what you’re doing:\n\n\nComplete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. All download and part links below:\n\n— Matthew Carrigan (@carrigmat) January 28, 2025\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTraining it from scratch is a whole different $5.6 million story-DeepSeek-V3 Technical Report."
  },
  {
    "objectID": "blog/self-host_LLM/index.html#distillation",
    "href": "blog/self-host_LLM/index.html#distillation",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "Distillation",
    "text": "Distillation\nSo that’s when distillation comes in hand. Distillation is a machine learning technique that involves transferring knowledge from a large model to a smaller one, thus making it less demanding while trying to achieve similar performance.\nThese more efficient smaller models can still achieve near state-of-the-art performance for specific tasks, while solving high cost and complexity challenges of deploying Large Language Models in real-world scenarios (Table 1).\nHere’s another quote from DeepSeek:\n\n\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distil better smaller models in the future. Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n\n\n\n\n\n\nTable 1: Evaluation on distilled models Hugging Face DeepSeek-AI\n\n\n\n\n\n\n\n\n\n \nAIME 2024 pass@1\nAIME 2024 cons@64\nMATH-500 pass@1\nGPQA Diamond pass@1\nLiveCodeBench pass@1\nCodeForces rating\n\n\nModel\n \n \n \n \n \n \n\n\n\n\nGPT-4o-0513\n9.3\n13.4\n74.6\n49.9\n32.9\n759\n\n\nClaude-3.5-Sonnet-1022\n16.0\n26.7\n78.3\n65.0\n38.9\n717\n\n\no1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\n\n\nQwQ-32B-Preview\n44.0\n60.0\n90.6\n54.5\n41.9\n1316\n\n\nDeepSeek-R1-Distill-Qwen-1.5B\n28.9\n52.7\n83.9\n33.8\n16.9\n954\n\n\nDeepSeek-R1-Distill-Qwen-7B\n55.5\n83.3\n92.8\n49.1\n37.6\n1189\n\n\nDeepSeek-R1-Distill-Qwen-14B\n69.7\n80.0\n93.9\n59.1\n53.1\n1481\n\n\nDeepSeek-R1-Distill-Qwen-32B\n72.6\n83.3\n94.3\n62.1\n57.2\n1691\n\n\nDeepSeek-R1-Distill-Llama-8B\n50.4\n80.0\n89.1\n49.0\n39.6\n1205\n\n\nDeepSeek-R1-Distill-Llama-70B\n70.0\n86.7\n94.5\n65.2\n57.5\n1633\n\n\n\n\n\n\n\n\nI tried running the DeepSeek-R1-Distill-Llama-8B but it was a bit to slow for me, so I settled for the DeepSeek-R1-Distill-Qwen-7B. For reference my machine is a Legion 5 (Lenovo) laptop with a Nvidia RTX 3060 (6GB VRAM)."
  },
  {
    "objectID": "blog/self-host_LLM/index.html#hosting-an-llm",
    "href": "blog/self-host_LLM/index.html#hosting-an-llm",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "Hosting an LLM",
    "text": "Hosting an LLM\nThere are three main options for hosting an LLM:\n1. Set up and run it on your own machine.\n2. Set it up on a cloud service.\n3. Use an API provider with their proprietary setup.\nMy plan was to set everything up myself and I was particularly interested in running it on my own computer. Still, I explored some cloud options—provided they were free—as a potential alternative.\n\nCloud Services\nThe three major cloud services: Azure, AWS, and GCP offer similar free tiers, typically including around $300 in credits and a limited number of usage hours. For example, AWS EC2’s free tier provides up to 750 hours but is limited to small instances with 2 vCPUs and 1 GiB of memory, which aren’t powerful enough for this project.\nOracle Cloud’s “Always Free” tier is a much more promising option, offering ARM-based virtual machines with up to 4 cores, 24 GB of RAM, and 200 GB of storage. Even so, my priority was testing on my own machine, so I stuck with that. Regardless, it’s definitely worth keeping in mind for future projects!\n\n\nAPI Providers\nAs for API providers, Groq is quite appealing for personal use. It allows up to 1,000 requests per day for free on the DeepSeek-R1-Distill-LLaMA-70B a fairly big model with impressive benchmark results. It integrates easily with development frameworks like LangChain for building applications and provides a straightforward way to create user interfaces using Gradio or Streamlit, plus the speed is one of its key selling points.\nHugging Face also has Spaces hardware and Inference endpoint solutions, where the first allows for hardware rental in the form of a development space and the second for the deployment of your applications."
  },
  {
    "objectID": "blog/self-host_LLM/index.html#llm-inference-serving",
    "href": "blog/self-host_LLM/index.html#llm-inference-serving",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "LLM Inference Serving",
    "text": "LLM Inference Serving\nServing LLM-based applications involves two key components: the engine and the server. The engine manages model execution and request batching, while the server handles routing user requests.\n\n\n\nFig. 1 - “Architecture of servers and engines” Source: RunAi\n\n\n\nEngines\nEngines are responsible for running the models and generating text using various optimization techniques. At their core, they are typically built with Python or C++ libraries. They process incoming user requests in batches and generate responses accordingly.\n\n\nServer\nThe servers orchestrate HTTP/gRPC requests from users. In real-world applications, users interact with the chatbot at different times, the server queues these requests and sends them to the engine for response generation. Additionally, it monitors important performance metrics such as throughput and latency, essential for optimizing model serving.\nFor more on serving here’s a RunAi article.\nChoosing the right inference backend for serving LLMs plays a critical role in achieving fast generation speeds for a smooth user experience, while also boosting cost efficiency through high token throughput and optimized resource usage. With a wide range of inference engines and servers available from leading research and industry teams, selecting the best fit for a specific use case can be a challenging task.\nPopular open-source tools for inference serving:\n\nTriton Inference Server & TensorRT-LLM - NVIDIA\nvLLM - Originally Sky Computing Lab at UC Berkeley has evolved into a community-driven project with contributions from both academia and industry.\nTGI - Hugging Face\nOllama - Community driven.\nAphrodite-Engine - PygmalionAI & Ruliad\nLMDeploy - MMRazor & MMDeploy\nSGLang - Backed by an active community with industry adoption.\nllama.cpp - Started from ggml.ai\nRayLLM & RayServe - Anyscale\n\n\n\nOllama\n\nI went with Ollama for this project due to its simplicity, accessibility, and smooth integration with various frameworks—ensuring it’ll be straightforward to implement future features, such as the frontend which I’ll discuss in the next section.\n\n\n\n\n\n\nTip\n\n\n\nI’ve already looked into it beforehand and I know it’ll make setting up fine-tuning with RAG and Unsloth-AI in a future project much easier. 😎\n\n\nOllama provides a constantly updated library of pre-trained LLM models, while ensuring effortless model management eliminating the complexities of model formats and dependencies. While it may not be the most scalable solution for large enterprises and can be slower than some alternatives, it significantly simplifies environment setup and overall workflow.\nIt’s built on top of llama.cpp and employs a Client-Server architecture, where the client interacts with the user via the command line, and communication between the client, server, and engine happens over HTTP. The server can be started through the command line, a desktop application or docker. Regardless of the method they all invoke the same executable file.\n\n\n\nFig. 2- General Overview of Ollama. Adapted"
  },
  {
    "objectID": "blog/self-host_LLM/index.html#user-interface-ui",
    "href": "blog/self-host_LLM/index.html#user-interface-ui",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "User Interface (UI)",
    "text": "User Interface (UI)\nNow, we need a way to interact with our chatbot beyond the command line. The good news is that there are many open-source platforms with built-in interfaces that we can easily connect to our service. This means we don’t need to be developers to have an attractive and functional interface. In fact, the available solutions are so polished that there’s really no reason to build something from scratch that wouldn’t match the quality of these options.\nSome examples are:\n\nOpenWebUI\nHuggingChat\nAnythingLLM\nLibreChat\nJan\nText Generation WebUI\n\nthe list could go on and on…\n\nOpen WebUI\nI chose OpenWeb UI, and honestly, there’s not much to say—it’s just a fantastic tool all around. The interface is clean and intuitive, setting it up is easy, it offers extensive customization and features, it supports multiple models and backend and integrates advanced functionalities.\nOpen WebUI is a community driven self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for Retrieval Augmented Generation (RAG), making it a powerful AI deployment solution.\n\n\n\nFig. 3- Demo OpenWebUI."
  },
  {
    "objectID": "blog/self-host_LLM/index.html#tunneling-tools",
    "href": "blog/self-host_LLM/index.html#tunneling-tools",
    "title": "Setting up a self-hosted LLM chatbot ft. DeepSeek",
    "section": "Tunneling Tools",
    "text": "Tunneling Tools\nA tunneling tool allows you to expose a local service (running on your computer or private network) to the internet. It creates a secure tunnel between your local machine and a public URL, letting others access your local service without needing to configure complex networking settings like port forwarding or firewall rules.\nHow it works:\n1. The tunneling tool runs on your local machine and connects to an external server.\n2. The external server provides a public URL (often temporary).\n3. Requests to the public URL are forwarded through the tunnel to your local service.\n\nngrok\nNgrok is one of the most widely used tunneling tools, offering a quick and easy way to expose local services to the internet. With a single command like ngrok http 3000, it generates a public URL that forwards traffic to your local server, making it ideal for testing webhooks, remote access, and development. Paid plans offer additional features like custom domains, authentication, and enhanced security.\nFor more information on the topic specifically for our use case here’s a nice piece on ngrok blog worth checking out.\n\n\n\n\n\n\nWarning\n\n\n\nIn this post we are skipping security best-practices, to learn how to ensure you are using ngrok securely please check their documentation. You can also setup permissions and user groups on Open WebUI."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Participations",
    "section": "",
    "text": "Silva, V., Almeida, F., Carvalho, J.A. et al. “First report of linezolid-resistant cfrpositive methicillin-resistant Staphylococcus aureus in humans in Portugal.” Journal of Global Antimicrobial Resistance, vol. 17, pp. 323–325 (2019). https://doi:10.1016/j.jgar.2019.05.017  \nSilva, V., Almeida, F.., Carvalho, J.A. et al. “Emergence of community-acquired methicillin-resistant Staphylococcus aureus EMRSA-15 clone as the predominant cause of diabetic foot ulcer infections in Portugal.”  Eur J Clin Microbiol Infect Dis 39, 179–186 (2020). https://doi.org/10.1007/s10096-019-03709-6"
  },
  {
    "objectID": "publications.html#international-journals",
    "href": "publications.html#international-journals",
    "title": "Participations",
    "section": "",
    "text": "Silva, V., Almeida, F., Carvalho, J.A. et al. “First report of linezolid-resistant cfrpositive methicillin-resistant Staphylococcus aureus in humans in Portugal.” Journal of Global Antimicrobial Resistance, vol. 17, pp. 323–325 (2019). https://doi:10.1016/j.jgar.2019.05.017  \nSilva, V., Almeida, F.., Carvalho, J.A. et al. “Emergence of community-acquired methicillin-resistant Staphylococcus aureus EMRSA-15 clone as the predominant cause of diabetic foot ulcer infections in Portugal.”  Eur J Clin Microbiol Infect Dis 39, 179–186 (2020). https://doi.org/10.1007/s10096-019-03709-6"
  },
  {
    "objectID": "publications.html#oral-communications",
    "href": "publications.html#oral-communications",
    "title": "Participations",
    "section": "Oral communications",
    "text": "Oral communications\nMultiple oral communications and posters at national and international congresses:\n\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2023). “WGS revolution: Maximizing variant detection with a multi-caller approach for CNVs and SNVs.”\nIV International Meeting of the Portuguese Society of Genetics. Instituto de Investigação e Inovação em Saúde (i3S), October 19th and 20th 2023.\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2023). Enhancing Genome Analysis: A Comprehensive WGS Pipeline for CNV and SNV Calling.\n6th PhDay: “Brilliant PhDs, brighter future: Shaping Science together!”, October 3rd and 4th 2023.\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2022). Whole genome sequencing analysis: exploring germline CNV landscapes.\n25th Annual Meeting of the Portuguese Society of Human Genetics. 18th-19th November (online event).\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2021). Whole genome sequencing analysis: exploring germline CNV and SNV landscapes.\n4th PhDay: “Break borders and build opportunities”. Instituto de Investigação e Inovação em Saúde (i3S) and online, 30th November 2021.\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2021). Whole genome sequencing analysis: exploring germline CNV and SNV landscapes.\nGenomePT Symposium 2021. July 7th (online event).\nVanessa Silva, Francisco Almeida, José António Carvalho, Ana Paula Silva, Manuela Canica, Justina Prada, Isabel Pires, Luis Maltez, José Eduardo Pereira, Gilberto Igrejas, Patrícia Poeta (2018). Prevalence of biofilm-related genes in Staphylococcus aureus isolated from infected diabetic foot ulcers.\nOral communication presented in 2nd Biochemistry and Microbiology Applied Technologies Conference, Hammamet, Tunisia.\nVanessa Silva, Francisco Almeida, Soraia Oliveira, Vera Managoeiro, Eugénia Ferreira, José Eduardo Pereira, Manuela Canica, Gilberto Igrejas, Patrícia Poeta (2019).\nLinezolid resistant cfr-positive methicillin-resistant Staphylococcus aureus causing infection in infected diabetic foot ulcers.\nOral communication presented at International Congress of Antimicrobial Resistance, IC2AR 2019, Abstract book 170, Caparica, Portugal."
  },
  {
    "objectID": "publications.html#posters",
    "href": "publications.html#posters",
    "title": "Participations",
    "section": "Posters",
    "text": "Posters\nMultiple national and International poster presentations having received an honorable mention of Best Poster in Congresso da Ciência – Conhecimentos D’Ouro [4] :\n\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2023). Genomic insights unveiled: A comprehensive WGS pipeline for CNV and SNV discovery.\nIII ASPIC-ASEICA International Meeting. Instituto de Investigação e Inovação em Saúde (i3S), October 26th and 26th 2023.\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2022). Whole genome sequencing analysis: the future in healthcare.\n26th Annual Meeting of the Portuguese Society of Human Genetics. Coimbra, Portugal, 17th-19th November 2022.\nMarta Ferreira, Celina São José, Francisco Almeida, Joaquin Jurado Maqueda, Rita Monteiro, Pedro Ferreira, Carla Oliveira (2022). Detection of germline variants by Whole genome sequencing analysis.\n28th Porto Cancer Meeting – Extracellular Vesicles, Cell Communication and Cancer.\nInstituto de Investigação e Inovação em Saúde (i3S) and online, 12th-13th May 2022.\nFrancisco Almeida, Vanessa Silva, José António Carvalho, Ana Paula Silva, Gilberto Igrejas, Patrícia Poeta (2019).\nPhenotypic and genotypic characterization of clinical isolates of methicillin resistant Staphylococcus aureus.\nPoster presented in Congresso da Ciência – Conhecimentos D’Ouro, Abstract book p. 21, Vila Real, Portugal.\nFrancisco Almeida, Vanessa Silva, José Eduardo Pereira, Gilberto Igrejas, Patrícia Poeta (2019). Analysis of virulence genes and agr types among methicillin-resistant Staphylococcus aureus from infected diabetic foot ulcers.\nPoster presented in 3rd International Caparica Conference in Antibiotic Resistance, IC2AR 2019, Abstract book p., Caparica, Portugal.\nVanessa Silva,Francisco Almeida, Soraia Oliveira, Vera Manageiro, Eugénia Ferreira, José Eduardo Pereira, Manuela Caniça, Gilberto Igrejas, Patrícia Poeta (2019)\nLinezolid resistant cfr-positive methicillinresistant Staphylococcus aureus isolated from infected diabetic foot ulcers.\nPoster presented in 3rd International Caparica Conference in Antibiotic Resistance, IC2AR 2019, Abstract book p.170, Caparica, Portugal.\nFrancisco Almeida,Vanessa Silva, Gilberto Igrejas, Patrícia Poeta (2018) Caracterização fenotípica deresistência a antibióticos em MRSA de origem clínica do norte de Portugal.\nPoster presented in 3rd International Caparica Conference in Antibiotic Resistance, IC2AR 2019, Abstract book p.170, Caparica, Portugal.\nVanessa Silva,Francisco Almeida, Gilberto Igrejas, Patrícia Poeta (2018). Prevalence and phenotypic characterization of clinical isolates of methicillin-resistant Staphylococcus aureus from Portugal\nPoster presented in International Summit on Microbiology and Parasitology, Abstract book p.37, Praga, Czech Republic"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "My First PC Build\n\n\nMy 3rd laptop in 10 years died, I went and build a small desktop (μATX).\n\n\n\n\n\n\n06 April 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Francisco de Almeida",
    "section": "",
    "text": "Hey there, thank you for visiting my website! 👋\nI worked as a data scientist and bioinformatics researcher. Currently, I took some time from business to learn more about topics that fascinate me.\nMy interests span generally in the entire machine learning lifecycle with a particular focus on scalable infrastructures, deep learning, and applications in biotechnology. I am also enthusiastic about bioinformatics and data visualization.\nThis place serves as my personal portfolio for projects and readings so feel free to comment or contact me directly via mail, every feedback is welcome!\n\n \n  \n   \n  \n    \n     linkedin\n  \n  \n    \n     github\n  \n  \n    \n     email\n  \n  \n    \n     resume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Setting up a self-hosted LLM chatbot ft. DeepSeek\n          \n        \n      \n    \n      \n        Self-host a model, serve, provide an UI, and expose it.\n      \n\n      \n        06 March 2025\n      \n  \n      \n        LLMs\n      \n\n    \n    \n  \n  \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/pc_build/index.html",
    "href": "til/pc_build/index.html",
    "title": "My First PC Build",
    "section": "",
    "text": "Case closed\n\n\n\n\n\n\n\nLeft open\n\n\n\n\n\n\n\nBackside\n\n\n\n\n\nSpecs: PcPartPicker\nLooks damn good and it was damn hard, I really tried my best to double-check every decision on placement, installation and cable management, ended up having to rebuild a couple of times mainly because of the liquid cooler placement.\nOverall I am really proud of what I achieved with 1568€, highlights on: - Liquid cooler AIO 360 pure loop2 fx reconditioned for half price only had very small bent fins, that’s why I insisted so much in keeping it altought it made my life miserable (fan hub + pump on top). - Rtx 3090 2nd hand for 650€ (24gb VRAM great for training and fine tunning models). - Ryzen 7700 tray version for 200 €.\nI also think the build is quite upgradable in the future.\n\n\n\n\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRadiator Back\n\n\n\n\n\n\n\nRadiator Front\n\n\n\n\n\n\n\n\n\nRTX 3090 Gigabyte OC\n\n\n\n\n\n\n\nRyzen 7700 Tray version\n\n\n\n\n\n\n\n\nHere’s a timelapse"
  }
]
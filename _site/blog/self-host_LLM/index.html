<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-06">
<meta name="description" content="Self-host a model, serve, provide an UI, and expose it.">

<title>Setting up a self-hosted LLM chatbot ft. DeepSeek ‚Äì Francisco de Almeida</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-031e4cdb8b2a37c9782ef7b4311f43f6.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-9d4d5890cf753409e64b708a5bd3985d.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6e998b2a873d0d2df80fc94a9c1091a1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-9deb59a7f4d65845a7fdf4189cd1976b.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Francisco de Almeida</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html"> 
<span class="menu-text">til</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/index.html"> 
<span class="menu-text">notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">about</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Setting up a self-hosted LLM chatbot ft. DeepSeek</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLMs</div>
  </div>
  </div>

<div>
  <div class="description">
    Self-host a model, serve, provide an UI, and expose it.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 6, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/extended_post_llms.png" class="img-fluid figure-img"></p>
<figcaption><em>Canva GenAI, prompt: ‚ÄúMinimalistic llama riding a whale with a laptop and some reference to connection to a chatbot, deepseek logo somewhere.‚Äù</em></figcaption>
</figure>
</div>
<p><br></p>
<p>Over the past few days I‚Äôve been diving into multiple LLM frameworks, exploring the best ways to deploy a model on my own machine with the goal of setting up an AI inference chatbot for free that I can use any-time and anywhere and even make it available to others.</p>
<p>I am very proud of my 3-year-old laptop that ended up hosting a deepseek model on his gpu and hold up quite well!</p>
<p>If you search this topic, you will find many tutorials on how to run models locally, setup a LLM in known cloud services or even use API providers that hosts LLMs in their proprietary setup</p>
<p>On this post I summarize the most common approaches I came across, the limitations I‚Äôve found and different setup iterations depending on your needs. I also walk through how I self-hosted a model, served it, gave-it an UI and exposed it. Honestly, it‚Äôs quite easy, there are so many great resources available.</p>
<p><br></p>
<section id="deepseek" class="level1 page-columns page-full">
<h1>DeepSeek</h1>
<section id="base-models" class="level2">
<h2 class="anchored" data-anchor-id="base-models">Base Models</h2>
<p><img height="64" src="https://unpkg.com/@lobehub/icons-static-svg@latest/icons/deepseek-color.svg"></p>
<p>What‚Äôs the buzz surrounding DeepSeek all about? It delivers strong performance, occasionally outpacing competitors while consistently holding its own‚Äîbut what truly distinguishes it from previous models?</p>
<p>In a nutshell <a href="https://arxiv.org/abs/2501.12948">Deepseek R1-Zero / R1</a> is introduced as the <strong>first-generation reasoning models</strong>, unlike the competitors these models articulate their reasoning behind every answer, step by step. This <strong>Chain-of-Tought (CoT)</strong> is great not only for the user but also for the model which is aware of the reasoning and its capable of learn and correct it if needed. By applying <strong>Reinforcement Learning (RL)</strong> the model gets better over time, trough experimentation and evaluation DeepSeek models are capable of improving their reasoning and update their behaviour. As a result, the need for massive amounts of labelled data is also reduced.</p>
<p>Here are two quotes from DeepSeek:</p>
<blockquote class="blockquote">
<p>
<em>We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.</em>
</p><p>
</p><p>
<em>We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.</em>
</p><p>
</p><p>
<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><em>Hugging Face DeepSeek-AI</em></a>
</p></blockquote>
<p><strong>Summarizing R1:</strong> uses a hybrid approach, employes <strong>Group Relative Policy Optimization (GRPO</strong>) as the optimization policy (<strong>RL</strong>), utilizes cold-start data in its initial training phase (<strong>SFT</strong>), and undergoes additional refinement stages.</p>
<p>Did I mention that <strong>DeepSeek-R1</strong> is open source? A heads-up‚Äîif you‚Äôre thinking you can run the full model on your own machine, think again.</p>
<p>Like other massive AI models, <strong>DeepSeek-R1 671B</strong> (with 671 billion parameters) requires a lot of computing power. Even though it doesn‚Äôt activate all 671 billion parameters at once, it still demands significant resources due to its sheer scale. To improve efficiency, it uses a <strong>Mixture-of-Experts (MoE)</strong> architecture, activating only 37 billion parameters per request. On top of that, it incorporates large-scale reinforcement learning and other optimizations that further enhance performance and efficiency.</p>
<p>And by ‚Äúa lot,‚Äù I don‚Äôt mean that much if you know what you‚Äôre doing:</p>
<blockquote class="twitter-tweet tw-align-center blockquote">
<p lang="en" dir="ltr">
Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. All download and part links below:
</p>
‚Äî Matthew Carrigan (<span class="citation" data-cites="carrigmat">@carrigmat</span>) <a href="https://twitter.com/carrigmat/status/1884244369907278106?ref_src=twsrc%5Etfw">January 28, 2025</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Training it from scratch is a whole different $5.6 million story-<a href="https://arxiv.org/html/2412.19437v1">DeepSeek-V3 Technical Report</a>.</p>
</div>
</div>
</section>
<section id="distillation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="distillation">Distillation</h2>
<p>So that‚Äôs when <strong>distillation</strong> comes in hand. <strong>Distillation is a machine learning technique that involves transferring knowledge from a large model to a smaller one, thus making it less demanding while trying to achieve similar performance.</strong></p>
<p>These more efficient smaller models can still achieve near state-of-the-art performance for specific tasks, while solving high cost and complexity challenges of deploying Large Language Models in real-world scenarios (<a href="#tbl-1" class="quarto-xref">Table&nbsp;1</a>).</p>
<p>Here‚Äôs another quote from DeepSeek:</p>
<blockquote class="blockquote">
<p>
<em>We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distil better smaller models in the future. Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</em>
</p><p>
</p><p>
</p></blockquote>
<p><br></p>
<div id="tbl-1" class="striped hover quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Evaluation on distilled models <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><em>Hugging Face DeepSeek-AI</em></a>
</figcaption>
<div aria-describedby="tbl-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<div id="49064348" class="cell page-columns page-full" data-execution_count="1">
<div class="cell-output cell-output-display column-body-outset" data-execution_count="1">
<style type="text/css">
#T_33b5e_row2_col5, #T_33b5e_row7_col0, #T_33b5e_row9_col1, #T_33b5e_row9_col2, #T_33b5e_row9_col3, #T_33b5e_row9_col4 {
  color: white;
  font-weight: bold;
  background-color: darkblue;
}
</style>

<table id="T_33b5e" class="table-striped table-hover caption-top table table-sm small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th class="blank level0" data-quarto-table-cell-role="th">&nbsp;</th>
<th id="T_33b5e_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">AIME 2024 pass@1</th>
<th id="T_33b5e_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">AIME 2024 cons@64</th>
<th id="T_33b5e_level0_col2" class="col_heading level0 col2" data-quarto-table-cell-role="th">MATH-500 pass@1</th>
<th id="T_33b5e_level0_col3" class="col_heading level0 col3" data-quarto-table-cell-role="th">GPQA Diamond pass@1</th>
<th id="T_33b5e_level0_col4" class="col_heading level0 col4" data-quarto-table-cell-role="th">LiveCodeBench pass@1</th>
<th id="T_33b5e_level0_col5" class="col_heading level0 col5" data-quarto-table-cell-role="th">CodeForces rating</th>
</tr>
<tr class="even">
<th class="index_name level0" data-quarto-table-cell-role="th">Model</th>
<th class="blank col0" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col1" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col2" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col3" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col4" data-quarto-table-cell-role="th">&nbsp;</th>
<th class="blank col5" data-quarto-table-cell-role="th">&nbsp;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_33b5e_level0_row0" class="row_heading level0 row0" data-quarto-table-cell-role="th">GPT-4o-0513</td>
<td id="T_33b5e_row0_col0" class="data row0 col0">9.3</td>
<td id="T_33b5e_row0_col1" class="data row0 col1">13.4</td>
<td id="T_33b5e_row0_col2" class="data row0 col2">74.6</td>
<td id="T_33b5e_row0_col3" class="data row0 col3">49.9</td>
<td id="T_33b5e_row0_col4" class="data row0 col4">32.9</td>
<td id="T_33b5e_row0_col5" class="data row0 col5">759</td>
</tr>
<tr class="even">
<td id="T_33b5e_level0_row1" class="row_heading level0 row1" data-quarto-table-cell-role="th">Claude-3.5-Sonnet-1022</td>
<td id="T_33b5e_row1_col0" class="data row1 col0">16.0</td>
<td id="T_33b5e_row1_col1" class="data row1 col1">26.7</td>
<td id="T_33b5e_row1_col2" class="data row1 col2">78.3</td>
<td id="T_33b5e_row1_col3" class="data row1 col3">65.0</td>
<td id="T_33b5e_row1_col4" class="data row1 col4">38.9</td>
<td id="T_33b5e_row1_col5" class="data row1 col5">717</td>
</tr>
<tr class="odd">
<td id="T_33b5e_level0_row2" class="row_heading level0 row2" data-quarto-table-cell-role="th">o1-mini</td>
<td id="T_33b5e_row2_col0" class="data row2 col0">63.6</td>
<td id="T_33b5e_row2_col1" class="data row2 col1">80.0</td>
<td id="T_33b5e_row2_col2" class="data row2 col2">90.0</td>
<td id="T_33b5e_row2_col3" class="data row2 col3">60.0</td>
<td id="T_33b5e_row2_col4" class="data row2 col4">53.8</td>
<td id="T_33b5e_row2_col5" class="data row2 col5">1820</td>
</tr>
<tr class="even">
<td id="T_33b5e_level0_row3" class="row_heading level0 row3" data-quarto-table-cell-role="th">QwQ-32B-Preview</td>
<td id="T_33b5e_row3_col0" class="data row3 col0">44.0</td>
<td id="T_33b5e_row3_col1" class="data row3 col1">60.0</td>
<td id="T_33b5e_row3_col2" class="data row3 col2">90.6</td>
<td id="T_33b5e_row3_col3" class="data row3 col3">54.5</td>
<td id="T_33b5e_row3_col4" class="data row3 col4">41.9</td>
<td id="T_33b5e_row3_col5" class="data row3 col5">1316</td>
</tr>
<tr class="odd">
<td id="T_33b5e_level0_row4" class="row_heading level0 row4" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Qwen-1.5B</td>
<td id="T_33b5e_row4_col0" class="data row4 col0">28.9</td>
<td id="T_33b5e_row4_col1" class="data row4 col1">52.7</td>
<td id="T_33b5e_row4_col2" class="data row4 col2">83.9</td>
<td id="T_33b5e_row4_col3" class="data row4 col3">33.8</td>
<td id="T_33b5e_row4_col4" class="data row4 col4">16.9</td>
<td id="T_33b5e_row4_col5" class="data row4 col5">954</td>
</tr>
<tr class="even">
<td id="T_33b5e_level0_row5" class="row_heading level0 row5" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Qwen-7B</td>
<td id="T_33b5e_row5_col0" class="data row5 col0">55.5</td>
<td id="T_33b5e_row5_col1" class="data row5 col1">83.3</td>
<td id="T_33b5e_row5_col2" class="data row5 col2">92.8</td>
<td id="T_33b5e_row5_col3" class="data row5 col3">49.1</td>
<td id="T_33b5e_row5_col4" class="data row5 col4">37.6</td>
<td id="T_33b5e_row5_col5" class="data row5 col5">1189</td>
</tr>
<tr class="odd">
<td id="T_33b5e_level0_row6" class="row_heading level0 row6" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Qwen-14B</td>
<td id="T_33b5e_row6_col0" class="data row6 col0">69.7</td>
<td id="T_33b5e_row6_col1" class="data row6 col1">80.0</td>
<td id="T_33b5e_row6_col2" class="data row6 col2">93.9</td>
<td id="T_33b5e_row6_col3" class="data row6 col3">59.1</td>
<td id="T_33b5e_row6_col4" class="data row6 col4">53.1</td>
<td id="T_33b5e_row6_col5" class="data row6 col5">1481</td>
</tr>
<tr class="even">
<td id="T_33b5e_level0_row7" class="row_heading level0 row7" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Qwen-32B</td>
<td id="T_33b5e_row7_col0" class="data row7 col0">72.6</td>
<td id="T_33b5e_row7_col1" class="data row7 col1">83.3</td>
<td id="T_33b5e_row7_col2" class="data row7 col2">94.3</td>
<td id="T_33b5e_row7_col3" class="data row7 col3">62.1</td>
<td id="T_33b5e_row7_col4" class="data row7 col4">57.2</td>
<td id="T_33b5e_row7_col5" class="data row7 col5">1691</td>
</tr>
<tr class="odd">
<td id="T_33b5e_level0_row8" class="row_heading level0 row8" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Llama-8B</td>
<td id="T_33b5e_row8_col0" class="data row8 col0">50.4</td>
<td id="T_33b5e_row8_col1" class="data row8 col1">80.0</td>
<td id="T_33b5e_row8_col2" class="data row8 col2">89.1</td>
<td id="T_33b5e_row8_col3" class="data row8 col3">49.0</td>
<td id="T_33b5e_row8_col4" class="data row8 col4">39.6</td>
<td id="T_33b5e_row8_col5" class="data row8 col5">1205</td>
</tr>
<tr class="even">
<td id="T_33b5e_level0_row9" class="row_heading level0 row9" data-quarto-table-cell-role="th">DeepSeek-R1-Distill-Llama-70B</td>
<td id="T_33b5e_row9_col0" class="data row9 col0">70.0</td>
<td id="T_33b5e_row9_col1" class="data row9 col1">86.7</td>
<td id="T_33b5e_row9_col2" class="data row9 col2">94.5</td>
<td id="T_33b5e_row9_col3" class="data row9 col3">65.2</td>
<td id="T_33b5e_row9_col4" class="data row9 col4">57.5</td>
<td id="T_33b5e_row9_col5" class="data row9 col5">1633</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</figure>
</div>
<p>I tried running the DeepSeek-R1-Distill-Llama-8B but it was a bit to slow for me, so I settled for the DeepSeek-R1-Distill-Qwen-7B. For reference my machine is a Legion 5 (Lenovo) laptop with a Nvidia RTX 3060 (6GB VRAM).</p>
<p><br></p>
</section>
</section>
<section id="serving-hosting" class="level1">
<h1>Serving &amp; Hosting</h1>
<section id="hosting-an-llm" class="level2">
<h2 class="anchored" data-anchor-id="hosting-an-llm">Hosting an LLM</h2>
<p>There are three main options for hosting an LLM:</p>
<p><strong>1.</strong> Set up and run it on your own machine.</p>
<p><strong>2.</strong> Set it up on a cloud service.</p>
<p><strong>3.</strong> Use an API provider with their proprietary setup.</p>
<p>My plan was to set everything up myself and I was particularly interested in running it on my own computer. Still, I explored some cloud options‚Äîprovided they were free‚Äîas a potential alternative.</p>
<section id="cloud-services" class="level3">
<h3 class="anchored" data-anchor-id="cloud-services">Cloud Services</h3>
<p>The three major cloud services: Azure, AWS, and GCP offer similar free tiers, typically including around $300 in credits and a limited number of usage hours. For example, AWS EC2‚Äôs free tier provides up to 750 hours but is limited to small instances with 2 vCPUs and 1 GiB of memory, which aren‚Äôt powerful enough for this project.</p>
<p>Oracle Cloud‚Äôs ‚ÄúAlways Free‚Äù tier is a much more promising option, offering ARM-based virtual machines with up to 4 cores, 24 GB of RAM, and 200 GB of storage. Even so, my priority was testing on my own machine, so I stuck with that. Regardless, it‚Äôs definitely worth keeping in mind for future projects!</p>
</section>
<section id="api-providers" class="level3">
<h3 class="anchored" data-anchor-id="api-providers">API Providers</h3>
<p>As for API providers, <strong>Groq</strong> is quite appealing for personal use. It allows up to 1,000 requests per day for free on the DeepSeek-R1-Distill-LLaMA-70B a fairly big model with impressive benchmark results. It integrates easily with development frameworks like <strong>LangChain</strong> for building applications and provides a straightforward way to create user interfaces using <strong>Gradio</strong> or <strong>Streamlit</strong>, plus the speed is one of its key selling points.</p>
<p><strong>Hugging Face</strong> also has <em>Spaces</em> hardware and <em>Inference endpoint</em> solutions, where the first allows for hardware rental in the form of a development space and the second for the deployment of your applications.</p>
<p><br></p>
</section>
</section>
<section id="llm-inference-serving" class="level2">
<h2 class="anchored" data-anchor-id="llm-inference-serving">LLM Inference Serving</h2>
<p>Serving LLM-based applications involves two key components: the <strong>engine</strong> and the <strong>server</strong>. The engine manages model execution and request batching, while the server handles routing user requests.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/image_run_ai.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Fig. 1 - ‚ÄúArchitecture of servers and engines‚Äù Source: RunAi"><img src="img/image_run_ai.png" class="img-fluid figure-img" alt="Fig. 1 - ‚ÄúArchitecture of servers and engines‚Äù Source: RunAi"></a></p>
<figcaption>Fig. 1 - <em>‚ÄúArchitecture of servers and engines‚Äù</em> Source: <a href="https://www.run.ai/blog/serving-large-language-models">RunAi</a></figcaption>
</figure>
</div>
<section id="engines" class="level3">
<h3 class="anchored" data-anchor-id="engines">Engines</h3>
<p>Engines are responsible for running the models and generating text using various optimization techniques. At their core, they are typically built with Python or C++ libraries. They process incoming user requests in batches and generate responses accordingly.</p>
</section>
<section id="server" class="level3">
<h3 class="anchored" data-anchor-id="server">Server</h3>
<p>The servers orchestrate HTTP/gRPC requests from users. In real-world applications, users interact with the chatbot at different times, the server queues these requests and sends them to the engine for response generation. Additionally, it monitors important performance metrics such as throughput and latency, essential for optimizing model serving.</p>
<p>For more on serving here‚Äôs a <a href="https://www.run.ai/blog/serving-large-language-models">RunAi article</a>.</p>
<p>Choosing the right inference backend for serving LLMs plays a critical role in achieving fast generation speeds for a smooth user experience, while also boosting cost efficiency through high token throughput and optimized resource usage. With a wide range of inference engines and servers available from leading research and industry teams, selecting the best fit for a specific use case can be a challenging task.</p>
<p>Popular open-source tools for inference serving:</p>
<ul>
<li><p><strong>Triton Inference Server</strong> &amp; <strong>TensorRT-LLM</strong> - NVIDIA</p></li>
<li><p><strong>vLLM</strong> - Originally Sky Computing Lab at UC Berkeley has evolved into a community-driven project with contributions from both academia and industry.</p></li>
<li><p><strong>TGI</strong> - Hugging Face</p></li>
<li><p><strong>Ollama</strong> - Community driven.</p></li>
<li><p><strong>Aphrodite-Engine</strong> - PygmalionAI &amp; Ruliad</p></li>
<li><p><strong>LMDeploy</strong> - MMRazor &amp; MMDeploy</p></li>
<li><p><strong>SGLang</strong> - Backed by an active community with industry adoption.</p></li>
<li><p><strong>llama.cpp</strong> - Started from ggml.ai</p></li>
<li><p><strong>RayLLM &amp; RayServe</strong> - Anyscale</p></li>
</ul>
</section>
<section id="ollama" class="level3">
<h3 class="anchored" data-anchor-id="ollama">Ollama</h3>
<p><img height="64" src="https://unpkg.com/@lobehub/icons-static-svg@latest/icons/ollama.svg"></p>
<p>I went with Ollama for this project due to its simplicity, accessibility, and smooth integration with various frameworks‚Äîensuring it‚Äôll be straightforward to implement future features, such as the frontend which I‚Äôll discuss in the next section.</p>
<div id="tip1" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>I‚Äôve already looked into it beforehand and I know it‚Äôll make setting up fine-tuning with RAG and Unsloth-AI in a future project much easier. üòé</p>
</div>
</div>
<p>Ollama provides a constantly updated library of pre-trained LLM models, while ensuring effortless model management eliminating the complexities of model formats and dependencies. While it may not be the most scalable solution for large enterprises and can be slower than some alternatives, it significantly simplifies environment setup and overall workflow.</p>
<p>It‚Äôs built on top of <strong>llama.cpp</strong> and employs a Client-Server architecture, where the client interacts with the user via the command line, and communication between the client, server, and engine happens over HTTP. The server can be started through the command line, a desktop application or docker. Regardless of the method they all invoke the same executable file.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/image_ollama_general.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Fig. 2- General Overview of Ollama. Adapted"><img src="img/image_ollama_general.png" class="img-fluid figure-img" alt="Fig. 2- General Overview of Ollama. Adapted"></a></p>
<figcaption>Fig. 2- General Overview of Ollama. <a href="https://medium.com/@rifewang/analysis-of-ollama-architecture-and-conversation-processing-flow-for-ai-llm-tool-ead4b9f40975">Adapted</a></figcaption>
</figure>
</div>
<p><br></p>
</section>
</section>
<section id="user-interface-ui" class="level2">
<h2 class="anchored" data-anchor-id="user-interface-ui">User Interface (UI)</h2>
<p>Now, we need a way to interact with our chatbot beyond the command line. The good news is that there are many open-source platforms with built-in interfaces that we can easily connect to our service. This means we don‚Äôt need to be developers to have an attractive and functional interface. In fact, the available solutions are so polished that there‚Äôs really no reason to build something from scratch that wouldn‚Äôt match the quality of these options.</p>
<p>Some examples are:</p>
<ul>
<li><p><strong>OpenWebUI</strong></p></li>
<li><p><strong>HuggingChat</strong></p></li>
<li><p><strong>AnythingLLM</strong></p></li>
<li><p><strong>LibreChat</strong></p></li>
<li><p><strong>Jan</strong></p></li>
<li><p><strong>Text Generation WebUI</strong></p></li>
</ul>
<p>the list could go on and on‚Ä¶</p>
<section id="open-webui" class="level3">
<h3 class="anchored" data-anchor-id="open-webui">Open WebUI</h3>
<p>I chose OpenWeb UI, and honestly, there‚Äôs not much to say‚Äîit‚Äôs just a fantastic tool all around. The interface is clean and intuitive, setting it up is easy, it offers extensive customization and features, it supports multiple models and backend and integrates advanced functionalities.</p>
<p>Open WebUI is a community driven self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for <strong>Retrieval Augmented Generation (RAG)</strong>, making it a powerful AI deployment solution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="gif/demo.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Fig. 3- Demo OpenWebUI."><img src="gif/demo.gif" class="img-fluid figure-img" alt="Fig. 3- Demo OpenWebUI."></a></p>
<figcaption>Fig. 3- Demo <a href="https://github.com/open-webui/open-webui">OpenWebUI</a>.</figcaption>
</figure>
</div>
<p><br></p>
</section>
</section>
</section>
<section id="exposing" class="level1">
<h1>Exposing</h1>
<section id="tunneling-tools" class="level2">
<h2 class="anchored" data-anchor-id="tunneling-tools">Tunneling Tools</h2>
<p>A tunneling tool allows you to expose a local service (running on your computer or private network) to the internet. It creates a secure tunnel between your local machine and a public URL, letting others access your local service without needing to configure complex networking settings like port forwarding or firewall rules.</p>
<p>How it works:</p>
<p><strong>1.</strong> The tunneling tool runs on your local machine and connects to an external server.</p>
<p><strong>2.</strong> The external server provides a public URL (often temporary).</p>
<p><strong>3.</strong> Requests to the public URL are forwarded through the tunnel to your local service.</p>
<section id="ngrok" class="level3">
<h3 class="anchored" data-anchor-id="ngrok">ngrok</h3>
<p>Ngrok is one of the most widely used tunneling tools, offering a quick and easy way to expose local services to the internet. With a single command like <code>ngrok http 3000</code>, it generates a public URL that forwards traffic to your local server, making it ideal for testing webhooks, remote access, and development. Paid plans offer additional features like custom domains, authentication, and enhanced security.</p>
<p>For more information on the topic specifically for our use case here‚Äôs a nice <a href="https://ngrok.com/blog-post/unlock-remote-ai-power-with-ngrok-a-game-changer-for-developers">piece on ngrok blog</a> worth checking out.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this post we are skipping security best-practices, to learn how to ensure you are using <strong>ngrok</strong> securely please check their <a href="https://ngrok.com/docs/guides/other-guides/securing-your-tunnels/">documentation</a>. You can also setup permissions and user groups on <a href="https://docs.openwebui.com/getting-started/env-configuration#chat-permissions">Open WebUI</a>.</p>
</div>
</div>
<p><br></p>
</section>
</section>
</section>
<section id="hands-on" class="level1">
<h1>Hands-on</h1>
<hr>
<p>When I first envisioned this project, I thought it would be a great way to sharpen my OOP skills in Python. Funny enough, I ended up not writing a single line of Python. Both Ollama and OpenWebUI provide maintained Docker images, so setting them up is as simple as running their respective containers and configuring communication between them via endpoints. Container orchestration is handled using Docker Compose.</p>
<p>If you‚Äôre planning to run the model on a GPU, you‚Äôll need to configure the enviroment and install the NVIDIA drivers. This process can be easily automated using <code>post-create command</code> in your <code>docker-compose.yaml</code> or <code>dockerfile</code>.</p>
<blockquote class="blockquote">
<p>üöÄ I‚Äôll be sharing the full code soon, but first, I want to play around with fine-tuning and see if I can integrate that into the project. <strong>Stay tuned for an update in my next post!</strong></p>
</blockquote>
<p><br></p>
<p>I‚Äôm using VS Code on WSL, so I‚Äôll be referring to Linux commands. With VS Code you can also easily set up a DevContainer for testing, experimenting with different frameworks, or simply test things out.</p>
<p>The first step is to ensure you have Docker (or a compatible container runtime).</p>
<p>Then, if you‚Äôre running the model on your NVIDIA GPU, to set up your environment for GPU acceleration use the following commands (installing with apt):</p>
<div style="font-size: 75%;">
<p>Source: <a href="https://hub.docker.com/r/ollama/ollama">Ollama documentation</a></p>
</div>
<p><strong>1.</strong> Configure the repository</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://nvidia.github.io/libnvidia-container/gpgkey <span class="dt">\</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">|</span> <span class="fu">sudo</span> gpg <span class="at">--dearmor</span> <span class="at">-o</span> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-s</span> <span class="at">-L</span> https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list <span class="dt">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">|</span> <span class="fu">sed</span> <span class="st">'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'</span> <span class="dt">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">|</span> <span class="fu">sudo</span> tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get update</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>2.</strong> Install the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation">NVIDIA Container Toolkit packages.</a></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install <span class="at">-y</span> nvidia-container-toolkit</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As for docker compose all you need to do is setup both services, for example as following:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>docker-compose.yaml</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="docker-compose.yaml" data-eval="FALSE"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">services</span><span class="kw">:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ollama</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> ollama/ollama:latest</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> 7869:11434</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> .:/workspace</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> ./ollama/ollama:/root/.ollama</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">container_name</span><span class="kw">:</span><span class="at"> ollama</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">pull_policy</span><span class="kw">:</span><span class="at"> always</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tty</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">restart</span><span class="kw">:</span><span class="at"> always</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">environment</span><span class="kw">:</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> OLLAMA_KEEP_ALIVE=24h</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">networks</span><span class="kw">:</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> llm_inference</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">deploy</span><span class="kw">:</span><span class="co">  #Only if you're running with GPU</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">reservations</span><span class="kw">:</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">devices</span><span class="kw">:</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="kw">-</span><span class="at"> </span><span class="fu">driver</span><span class="kw">:</span><span class="at"> nvidia</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">count</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">capabilities</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="at">gpu</span><span class="kw">]</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">open-webui</span><span class="kw">:</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at">  ghcr.io/open-webui/open-webui:main</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">container_name</span><span class="kw">:</span><span class="at"> open-webui</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> ./open-webui:/app/backend/data</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">depends_on</span><span class="kw">:</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> ollama</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> 8080:8080</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">environment</span><span class="kw">:</span><span class="co"> # https://docs.openwebui.com/getting-started/env-configuration#default_models</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> OLLAMA_BASE_URLS=http://host.docker.internal:7869 </span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> ENV=dev</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> WEBUI_AUTH=False</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> WEBUI_URL=http://localhost:8080</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> WEBUI_SECRET_KEY= wg55pp</span><span class="co"> #random secret key</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">extra_hosts</span><span class="kw">:</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="st">"host.docker.internal:host-gateway"</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">restart</span><span class="kw">:</span><span class="at"> unless-stopped</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">networks</span><span class="kw">:</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> llm_inference</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="fu">networks</span><span class="kw">:</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">llm_inference</span><span class="kw">:</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">external</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">workspace</span><span class="kw">:</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">driver</span><span class="kw">:</span><span class="at"> local</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To download the model, you can interact directly with the container and pull the model. You can easily check all available models on <a href="https://ollama.com/search">Ollama‚Äôs site</a>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> exec <span class="at">-it</span> ollama run deepseek-r1:7b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that everything is set up, we can simply run all the services and expose the port used by OpenWebUI through a tunneling tool, in this case ngrok. Some other options and providers are covered on <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md">Ollama‚Äôs FAQ.</a></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ngrok</span> http 8080 <span class="at">--host-header</span><span class="op">=</span><span class="st">"localhost:8080"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we were to scale this for multiple instances/users, Kubernetes should work with a very similar setup. I came across a helpful post on Medium that explains how to do it <a href="https://medium.com/@yuxiaojian/host-your-own-ollama-service-in-a-cloud-kubernetes-k8s-cluster-c818ca84a055">Host Your Own Ollama Service in a Cloud Kubernetes (K8s) Cluster</a> I haven‚Äôt read it thoroughly, but I think it‚Äôs worth noting.</p>
<p><br></p>
<section id="closing-notes" class="level3">
<h3 class="anchored" data-anchor-id="closing-notes">Closing Notes</h3>
<p>Since I started writing this post in late February and published it on 6th of March, three more models utilizing reinforcement learning have been released. One is <a href="https://qwenlm.github.io/blog/qwq-32b/">QwQ-32B</a> (just today!), and the other is <a href="https://x.ai/blog/grok-3">Grok 3</a>, both setting new benchmark records. The first is open-source, and there are expectations that Grok 3 will follow, probably in the future when a new model from X replaces it. <a href="https://openai.com/index/gpt-4-5-system-card/">GPT-4.5</a> is also available as a preview for paid subscribers and uses reinforcement learning as well.</p>
<p>DeepSeek‚Äôs work with reinforcement learning has laid the foundation for a new approach to LLMs‚Äîone that emphasizes both reinforcement learning and open-source access.</p>
<p>The future looks bright, and these advancements should be within reach for anyone eager to embrace this journey. It‚Äôs exciting to witness how quickly progress is being made with more and better open-source tools emerging every day.</p>
<p><br></p>
<blockquote class="blockquote">
<p><em>I hope this post provides a clear overview on inference basics</em></p>
</blockquote>


</section>
</section>

</main> <!-- /main -->

    <!-- ... -->
    <link rel="stylesheet" href="https://unpkg.com/@waline/client@v3/dist/waline.css">
  
  
    <!-- ... -->
    <div id="waline"></div>
    <script type="module">
      import { init } from 'https://unpkg.com/@waline/client@v3/dist/waline.js';
  
      init({
        el: '#waline',
        serverURL: 'https://sitecomments-virid.vercel.app',
        lang: 'en',
        pageview: true, // pageview statistics
        reaction: [
    'https://unpkg.com/@waline/emojis@1.3.0/tw/1f44d.png',
    'https://unpkg.com/@waline/emojis@1.3.0/tw/1f44f.png',
    'https://unpkg.com/@waline/emojis@1.3.0/tw/2753.png',
  ],

      });
    </script>
  
  
  
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Built with <a href="https://quarto.org/">Quarto <iconify-icon role="img" inline="" icon="simple-icons:quarto" aria-label="Quarto Logo" title="Quarto Logo"></iconify-icon></a>. Copyright ¬© Francisco de Almeida 2025.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>License <a href="https://creativecommons.org/licenses/by/4.0/deed.en"><iconify-icon role="img" inline="" icon="bi:cc-circle" aria-label="Icon cc-circle from bi Iconify.design set." title="Icon cc-circle from bi Iconify.design set."></iconify-icon> BY 4.0</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>